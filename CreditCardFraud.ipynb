{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM0X6qTnd9ccrlYcagkrel7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DevaYadhala-04/Credit-Card-Fraud/blob/main/CreditCardFraud.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIeGSocraZOT"
      },
      "outputs": [],
      "source": [
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense, LSTM, Conv1D, concatenate, Reshape, Dropout\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "# Function to load dataset\n",
        "def load_dataset(file_path):\n",
        "    return pd.read_csv(file_path)\n",
        "\n",
        "# Function to preprocess data\n",
        "def preprocess_data(df):\n",
        "    df = df.dropna(subset=['Class'])\n",
        "    X = df.drop(['Class'], axis=1)\n",
        "    y = df['Class']\n",
        "    return X, y\n",
        "\n",
        "# Function to handle class imbalance\n",
        "def handle_class_imbalance(X, y):\n",
        "    from imblearn.over_sampling import SMOTE\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X, y)\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# Function to scale data\n",
        "def scale_data(X):\n",
        "    scaler = StandardScaler()\n",
        "    return scaler.fit_transform(X)\n",
        "\n",
        "# Function to split data into training and testing sets\n",
        "def split_data(X, y, test_size):\n",
        "    return train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "# Function to define autoencoder model\n",
        "def define_autoencoder_model(input_dim, encoding_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    encoder = Dense(encoding_dim, activation='relu')(input_layer)\n",
        "    decoder = Dense(input_dim, activation='sigmoid')(encoder)\n",
        "    autoencoder = Model(inputs=input_layer, outputs=decoder)\n",
        "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return autoencoder\n",
        "\n",
        "# Function to train autoencoder model\n",
        "def train_autoencoder_model(autoencoder, X_train, epochs, batch_size):\n",
        "    checkpoint_path = \"autoencoder_weights.best.weights.h5\"\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True, save_weights_only=True, mode='min', verbose=1)\n",
        "    autoencoder.fit(X_train, X_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_train, X_train), callbacks=[EarlyStopping(patience=10), checkpoint])\n",
        "    return checkpoint_path\n",
        "\n",
        "# Function to define hybrid model\n",
        "def define_hybrid_model(input_dim):\n",
        "    input_layer = Input(shape=(input_dim,))\n",
        "    x = Reshape((input_dim, 1))(input_layer)\n",
        "    x_conv = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x)\n",
        "    x_conv = Conv1D(32, kernel_size=3, activation='relu', padding='same')(x_conv)\n",
        "    x_lstm = LSTM(32, return_sequences=True)(x)\n",
        "    x_lstm = LSTM(32, return_sequences=False)(x_lstm)\n",
        "    x_conv_reshape = Reshape((x_conv.shape[1]*x_conv.shape[2],))(x_conv)\n",
        "    x_concat = concatenate([x_conv_reshape, x_lstm])\n",
        "    x_dense = Dense(64, activation='relu')(x_concat)\n",
        "    x_dense = Dropout(0.2)(x_dense)\n",
        "    output_layer = Dense(1, activation='sigmoid')(x_dense)\n",
        "    hybrid_model = Model(inputs=input_layer, outputs=output_layer)\n",
        "    hybrid_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "    return hybrid_model\n",
        "\n",
        "# Function to train hybrid model\n",
        "def train_hybrid_model(hybrid_model, X_train, y_train, epochs, batch_size):\n",
        "    checkpoint_path = \"hybrid_model_weights.best.weights.h5\"\n",
        "    checkpoint = ModelCheckpoint(checkpoint_path, monitor='val_accuracy', save_best_only=True, save_weights_only=True, mode='max', verbose=1)\n",
        "    history = hybrid_model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, shuffle=True, validation_data=(X_train, y_train), callbacks=[EarlyStopping(patience=10), checkpoint])\n",
        "    return checkpoint_path, history\n",
        "\n",
        "# Function to evaluate hybrid model and visualize\n",
        "def evaluate_hybrid_model(hybrid_model, X_test, y_test, history, X_train, X_resampled, y_resampled):\n",
        "    # Plot training and validation accuracy first\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Loss')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Evaluate test accuracy\n",
        "    loss, accuracy = hybrid_model.evaluate(X_test, y_test, verbose=0)\n",
        "    print(f'Test Accuracy: {accuracy:.2f}')\n",
        "\n",
        "    y_pred_prob = hybrid_model.predict(X_test)\n",
        "\n",
        "    auc = roc_auc_score(y_test, y_pred_prob)\n",
        "    print(f'AUC: {auc:.2f}')\n",
        "\n",
        "    # Predict and compute confusion matrix\n",
        "    y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "    fraud_count = np.sum(y_pred)\n",
        "    non_fraud_count = len(y_pred) - fraud_count\n",
        "    print(f'Predicted Non-Fraud Transactions: {non_fraud_count}')\n",
        "    print(f'Predicted Fraud Transactions: {fraud_count}')\n",
        "\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    # Display confusion matrix\n",
        "    plt.figure(figsize=(6, 6))\n",
        "    plt.imshow(conf_matrix, cmap=plt.cm.Blues)\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.colorbar()\n",
        "    plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
        "    plt.yticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "\n",
        "    for i in range(2):\n",
        "        for j in range(2):\n",
        "            plt.text(j, i, conf_matrix[i, j], ha='center', va='center', color='red')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Print classification report\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    # Visualizing fraud vs non-fraud transactions\n",
        "    results = pd.DataFrame({'Actual': y_test.values, 'Predicted': y_pred.flatten()})\n",
        "    fraud_transactions = results[results['Predicted'] == 1]\n",
        "    non_fraud_transactions = results[results['Predicted'] == 0]\n",
        "\n",
        "    print(\"Fraud Transactions:\\n\", fraud_transactions)\n",
        "    print(\"Non-Fraud Transactions:\\n\", non_fraud_transactions)\n",
        "\n",
        "    # Visualize the balance of the dataset\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    y_resampled.value_counts().plot(kind='bar')\n",
        "    plt.title('Class Distribution')\n",
        "    plt.xlabel('Class')\n",
        "    plt.ylabel('Count')\n",
        "    plt.xticks([0, 1], ['Non-Fraud', 'Fraud'])\n",
        "    plt.show()\n",
        "\n",
        "    # PCA + t-SNE for visualization\n",
        "    X_combined = np.concatenate((X_train, X_test), axis=0)\n",
        "    y_combined = np.concatenate((y_resampled, y_test), axis=0)\n",
        "\n",
        "    sample_size = 1000  # Adjust this size based on your machine's capabilities\n",
        "    np.random.seed(42)\n",
        "    sample_indices = np.random.choice(X_combined.shape[0], sample_size, replace=False)\n",
        "    X_sample = X_combined[sample_indices]\n",
        "    y_sample = y_combined[sample_indices]\n",
        "\n",
        "    # Perform PCA to reduce dimensionality before t-SNE\n",
        "    pca = PCA(n_components=30)  # Reduce to 30 components before applying t-SNE\n",
        "    X_pca = pca.fit_transform(X_sample)\n",
        "\n",
        "    # Perform t-SNE for 3D visualization\n",
        "    tsne = TSNE(n_components=3, random_state=42, perplexity=30)\n",
        "    X_tsne = tsne.fit_transform(X_pca)\n",
        "\n",
        "    # Plot the results\n",
        "    fig = plt.figure(figsize=(10, 7))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "\n",
        "    # Plot non-fraud transactions\n",
        "    ax.scatter(X_tsne[y_sample == 0, 0], X_tsne[y_sample == 0, 1], X_tsne[y_sample == 0, 2],\n",
        "               c='blue', label='Non-Fraud', alpha=0.5)\n",
        "\n",
        "    # Plot fraud transactions\n",
        "    ax.scatter(X_tsne[y_sample == 1, 0], X_tsne[y_sample == 1, 1], X_tsne[y_sample == 1, 2],\n",
        "               c='red', label='Fraud', alpha=0.5)\n",
        "\n",
        "    ax.set_title('3D t-SNE Visualization of Fraudulent Transactions')\n",
        "    ax.set_xlabel('t-SNE Feature 1')\n",
        "    ax.set_ylabel('t-SNE Feature 2')\n",
        "    ax.set_zlabel('t-SNE Feature 3')\n",
        "    ax.legend()\n",
        "    plt.show()\n",
        "\n",
        "# Main function\n",
        "def main():\n",
        "    file_path = input(\"Enter the file path of the dataset: \")\n",
        "    df = load_dataset(file_path)\n",
        "    X, y = preprocess_data(df)\n",
        "    X_resampled, y_resampled = handle_class_imbalance(X, y)  # y_resampled is defined here\n",
        "    X_scaled = scale_data(X_resampled)\n",
        "    test_size = float(input(\"Enter the test size (between 0 and 1): \"))\n",
        "    X_train, X_test, y_train, y_test = split_data(X_scaled, y_resampled, test_size)\n",
        "\n",
        "    encoding_dim = int(input(\"Enter the encoding dimension for the autoencoder: \"))\n",
        "    autoencoder = define_autoencoder_model(X_train.shape[1], encoding_dim)\n",
        "    autoencoder_epochs = int(input(\"Enter the number of epochs to train the autoencoder: \"))\n",
        "    autoencoder_batch_size = int(input(\"Enter the batch size to train the autoencoder: \"))\n",
        "    autoencoder_checkpoint_path = train_autoencoder_model(autoencoder, X_train, autoencoder_epochs, autoencoder_batch_size)\n",
        "\n",
        "    hybrid_model = define_hybrid_model(X_train.shape[1])\n",
        "    hybrid_epochs = int(input(\"Enter the number of epochs to train the hybrid model: \"))\n",
        "    hybrid_batch_size = int(input(\"Enter the batch size to train the hybrid model: \"))\n",
        "    hybrid_checkpoint_path, history = train_hybrid_model(hybrid_model, X_train, y_train, hybrid_epochs, hybrid_batch_size)\n",
        "\n",
        "    hybrid_model.load_weights(hybrid_checkpoint_path)\n",
        "    evaluate_hybrid_model(hybrid_model, X_test, y_test, history, X_train, X_resampled, y_resampled)\n",
        "\n",
        "if __name__ == \"main\":\n",
        "  main()"
      ]
    }
  ]
}